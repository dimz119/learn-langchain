{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-anthropic==0.3.10 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (0.3.10)\n",
      "Collecting langchain-community==0.3.21\n",
      "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-openai==0.3.12\n",
      "  Downloading langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langgraph==0.3.25\n",
      "  Downloading langgraph-0.3.25-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting langsmith==0.3.24\n",
      "  Downloading langsmith-0.3.24-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting tavily-python==0.5.4\n",
      "  Downloading tavily_python-0.5.4-py3-none-any.whl.metadata (91 kB)\n",
      "Requirement already satisfied: anthropic<1,>=0.49.0 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from langchain-anthropic==0.3.10) (0.50.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from langchain-anthropic==0.3.10) (0.3.55)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from langchain-anthropic==0.3.10) (2.11.3)\n",
      "Collecting langchain<1.0.0,>=0.3.23 (from langchain-community==0.3.21)\n",
      "  Downloading langchain-0.3.24-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain-community==0.3.21)\n",
      "  Downloading sqlalchemy-2.0.40-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from langchain-community==0.3.21) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from langchain-community==0.3.21) (6.0.2)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community==0.3.21)\n",
      "  Downloading aiohttp-3.11.18-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from langchain-community==0.3.21) (9.1.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.21)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.21)\n",
      "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community==0.3.21)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting numpy<3,>=1.26.2 (from langchain-community==0.3.21)\n",
      "  Downloading numpy-2.2.5-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting openai<2.0.0,>=1.68.2 (from langchain-openai==0.3.12)\n",
      "  Downloading openai-1.75.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai==0.3.12)\n",
      "  Downloading tiktoken-0.9.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph==0.3.25)\n",
      "  Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph==0.3.25)\n",
      "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph==0.3.25)\n",
      "  Downloading langgraph_sdk-0.1.63-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph==0.3.25)\n",
      "  Downloading xxhash-3.5.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from langsmith==0.3.24) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from langsmith==0.3.24) (3.10.16)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from langsmith==0.3.24) (24.2)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from langsmith==0.3.24) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from langsmith==0.3.24) (0.23.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21)\n",
      "  Downloading frozenlist-1.6.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21)\n",
      "  Downloading multidict-6.4.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21)\n",
      "  Downloading propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21)\n",
      "  Downloading yarl-1.20.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from anthropic<1,>=0.49.0->langchain-anthropic==0.3.10) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from anthropic<1,>=0.49.0->langchain-anthropic==0.3.10) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from anthropic<1,>=0.49.0->langchain-anthropic==0.3.10) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from anthropic<1,>=0.49.0->langchain-anthropic==0.3.10) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from anthropic<1,>=0.49.0->langchain-anthropic==0.3.10) (4.13.2)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.21)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.21)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: certifi in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith==0.3.24) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith==0.3.24) (1.0.8)\n",
      "Requirement already satisfied: idna in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith==0.3.24) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith==0.3.24) (0.14.0)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain<1.0.0,>=0.3.23->langchain-community==0.3.21)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-anthropic==0.3.10) (1.33)\n",
      "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph==0.3.25)\n",
      "  Downloading ormsgpack-1.9.1-cp313-cp313-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (43 kB)\n",
      "Collecting tqdm>4 (from openai<2.0.0,>=1.68.2->langchain-openai==0.3.12)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-anthropic==0.3.10) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-anthropic==0.3.10) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-anthropic==0.3.10) (0.4.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.21)\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from requests<3,>=2->langchain-community==0.3.21) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from requests<3,>=2->langchain-community==0.3.21) (2.4.0)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain-openai==0.3.12)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/seungjoonlee/.pyenv/versions/3.13.1/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-anthropic==0.3.10) (3.0.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.21)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.3.12-py3-none-any.whl (61 kB)\n",
      "Downloading langgraph-0.3.25-py3-none-any.whl (142 kB)\n",
      "Downloading langsmith-0.3.24-py3-none-any.whl (352 kB)\n",
      "Downloading tavily_python-0.5.4-py3-none-any.whl (44 kB)\n",
      "Downloading aiohttp-3.11.18-cp313-cp313-macosx_11_0_arm64.whl (454 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain-0.3.24-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langgraph_checkpoint-2.0.24-py3-none-any.whl (42 kB)\n",
      "Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
      "Downloading langgraph_sdk-0.1.63-py3-none-any.whl (47 kB)\n",
      "Downloading numpy-2.2.5-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.75.0-py3-none-any.whl (646 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.0/647.0 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Downloading sqlalchemy-2.0.40-cp313-cp313-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.9.0-cp313-cp313-macosx_11_0_arm64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp313-cp313-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.6.0-cp313-cp313-macosx_11_0_arm64.whl (120 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.4.3-cp313-cp313-macosx_11_0_arm64.whl (36 kB)\n",
      "Downloading ormsgpack-1.9.1-cp313-cp313-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (383 kB)\n",
      "Downloading propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl (44 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.20.0-cp313-cp313-macosx_11_0_arm64.whl (94 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: xxhash, tqdm, SQLAlchemy, regex, python-dotenv, propcache, ormsgpack, numpy, mypy-extensions, multidict, marshmallow, httpx-sse, frozenlist, attrs, aiohappyeyeballs, yarl, typing-inspect, tiktoken, aiosignal, tavily-python, pydantic-settings, openai, langsmith, langgraph-sdk, dataclasses-json, aiohttp, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langgraph-prebuilt, langchain, langgraph, langchain-community\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.3.33\n",
      "    Uninstalling langsmith-0.3.33:\n",
      "      Successfully uninstalled langsmith-0.3.33\n",
      "Successfully installed SQLAlchemy-2.0.40 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 attrs-25.3.0 dataclasses-json-0.6.7 frozenlist-1.6.0 httpx-sse-0.4.0 langchain-0.3.24 langchain-community-0.3.21 langchain-openai-0.3.12 langchain-text-splitters-0.3.8 langgraph-0.3.25 langgraph-checkpoint-2.0.24 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.63 langsmith-0.3.24 marshmallow-3.26.1 multidict-6.4.3 mypy-extensions-1.1.0 numpy-2.2.5 openai-1.75.0 ormsgpack-1.9.1 propcache-0.3.1 pydantic-settings-2.9.1 python-dotenv-1.1.0 regex-2024.11.6 tavily-python-0.5.4 tiktoken-0.9.0 tqdm-4.67.1 typing-inspect-0.9.0 xxhash-3.5.0 yarl-1.20.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-anthropic==0.3.10 langchain-community==0.3.21 langchain-openai==0.3.12 langgraph==0.3.25 langsmith==0.3.24 tavily-python==0.5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import deque\n",
    "from typing import Optional\n",
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage\n",
    "\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reflection(BaseModel):\n",
    "    reflections: str = Field(\n",
    "        description=\"The critique and reflections on the sufficiency, superfluency,\"\n",
    "        \" and general quality of the response\"\n",
    "    )\n",
    "    score: int = Field(\n",
    "        description=\"Score from 0-10 on the quality of the candidate response.\",\n",
    "        gte=0,\n",
    "        lte=10,\n",
    "    ) # type: ignore\n",
    "    found_solution: bool = Field(\n",
    "        description=\"Whether the response has fully solved the question or task.\"\n",
    "    )\n",
    "\n",
    "    def as_message(self):\n",
    "        return HumanMessage(\n",
    "            content=f\"Reasoning: {self.reflections}\\nScore: {self.score}\"\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def normalized_score(self) -> float:\n",
    "        return self.score / 10.0\n",
    "\n",
    "class Node:\n",
    "    def __init__(\n",
    "        self,\n",
    "        messages: list[BaseMessage],\n",
    "        reflection: Reflection,\n",
    "        parent: Optional[\"Node\"] = None,\n",
    "    ):\n",
    "        self.messages = messages\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.value = 0\n",
    "        self.visits = 0\n",
    "        self.reflection = reflection\n",
    "        self.depth = parent.depth + 1 if parent is not None else 1\n",
    "        self._is_solved = reflection.found_solution if reflection else False\n",
    "        if self._is_solved:\n",
    "            self._mark_tree_as_solved()\n",
    "        self.backpropagate(reflection.normalized_score)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"<Node value={self.value}, visits={self.visits},\"\n",
    "            f\" solution={self.messages} reflection={self.reflection}/>\"\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def is_solved(self):\n",
    "        \"\"\"If any solutions exist, we can end the search.\"\"\"\n",
    "        return self._is_solved\n",
    "\n",
    "    @property\n",
    "    def is_terminal(self):\n",
    "        return not self.children\n",
    "\n",
    "    @property\n",
    "    def best_child_score(self):\n",
    "        \"\"\"Return the child with the highest value.\"\"\"\n",
    "        if not self.children:\n",
    "            return None\n",
    "        return max(self.children, key=lambda child: int(child.is_solved) * child.value)\n",
    "\n",
    "    @property\n",
    "    def height(self) -> int:\n",
    "        \"\"\"Check for how far we've rolled out the tree.\"\"\"\n",
    "        if self.children:\n",
    "            return 1 + max([child.height for child in self.children])\n",
    "        return 1\n",
    "\n",
    "    def upper_confidence_bound(self, exploration_weight=1.0):\n",
    "        \"\"\"Return the UCT score. This helps balance exploration vs. exploitation of a branch.\"\"\"\n",
    "        if self.parent is None:\n",
    "            raise ValueError(\"Cannot obtain UCT from root node\")\n",
    "        if self.visits == 0:\n",
    "            return self.value\n",
    "        # Encourages exploitation of high-value trajectories\n",
    "        average_reward = self.value / self.visits\n",
    "        # Encourages exploration of less-visited trajectories\n",
    "        exploration_term = math.sqrt(math.log(self.parent.visits) / self.visits)\n",
    "        return average_reward + exploration_weight * exploration_term\n",
    "\n",
    "    def backpropagate(self, reward: float):\n",
    "        \"\"\"Update the score of this node and its parents.\"\"\"\n",
    "        node = self\n",
    "        while node:\n",
    "            node.visits += 1\n",
    "            node.value = (node.value * (node.visits - 1) + reward) / node.visits\n",
    "            node = node.parent\n",
    "\n",
    "    def get_messages(self, include_reflections: bool = True):\n",
    "        if include_reflections:\n",
    "            return self.messages + [self.reflection.as_message()]\n",
    "        return self.messages\n",
    "\n",
    "    def get_trajectory(self, include_reflections: bool = True) -> list[BaseMessage]:\n",
    "        \"\"\"Get messages representing this search branch.\"\"\"\n",
    "        messages = []\n",
    "        node = self\n",
    "        while node:\n",
    "            messages.extend(\n",
    "                node.get_messages(include_reflections=include_reflections)[::-1]\n",
    "            )\n",
    "            node = node.parent\n",
    "        # Reverse the final back-tracked trajectory to return in the correct order\n",
    "        return messages[::-1]  # root solution, reflection, child 1, ...\n",
    "\n",
    "    def _get_all_children(self):\n",
    "        all_nodes = []\n",
    "        nodes = deque()\n",
    "        nodes.append(self)\n",
    "        while nodes:\n",
    "            node = nodes.popleft()\n",
    "            all_nodes.extend(node.children)\n",
    "            for n in node.children:\n",
    "                nodes.append(n)\n",
    "        return all_nodes\n",
    "\n",
    "    def get_best_solution(self):\n",
    "        \"\"\"Return the best solution from within the current sub-tree.\"\"\"\n",
    "        all_nodes = [self] + self._get_all_children()\n",
    "        best_node = max(\n",
    "            all_nodes,\n",
    "            # We filter out all non-terminal, non-solution trajectories\n",
    "            key=lambda node: int(node.is_terminal and node.is_solved) * node.value,\n",
    "        )\n",
    "        return best_node\n",
    "\n",
    "    def _mark_tree_as_solved(self):\n",
    "        parent = self.parent\n",
    "        while parent:\n",
    "            parent._is_solved = True\n",
    "            parent = parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "class TreeState(TypedDict):\n",
    "    # The full tree\n",
    "    root: Node\n",
    "    # The original input\n",
    "    input: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "tools = [tavily_tool]\n",
    "tool_node = ToolNode(tools=tools)\n",
    "\n",
    "\n",
    "from langchain_core.output_parsers.openai_tools import (\n",
    "    JsonOutputToolsParser,\n",
    "    PydanticToolsParser,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import chain as as_runnable\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Reflect and grade the assistant response to the user question below.\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"candidate\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "reflection_llm_chain = (\n",
    "    prompt\n",
    "    | llm.bind_tools(tools=[Reflection], tool_choice=\"Reflection\").with_config(\n",
    "        run_name=\"Reflection\"\n",
    "    )\n",
    "    | PydanticToolsParser(tools=[Reflection])\n",
    ")\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def reflection_chain(inputs) -> Reflection:\n",
    "    tool_choices = reflection_llm_chain.invoke(inputs)\n",
    "    reflection = tool_choices[0]\n",
    "    if not isinstance(inputs[\"candidate\"][-1], AIMessage):\n",
    "        reflection.found_solution = False\n",
    "    return reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_deJYRta1OC207ayRQkKz0j1B', 'function': {'arguments': '{\"query\":\"lithium pollution environmental impact research 2023\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 93, 'total_tokens': 121, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_d8864f8b6b', 'id': 'chatcmpl-BPMKSBD5xdVxm9JuspQ5aIMvYuTvf', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0366564b-fb66-492f-9b4b-cf8dd2e17cd7-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'lithium pollution environmental impact research 2023'}, 'id': 'call_deJYRta1OC207ayRQkKz0j1B', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 28, 'total_tokens': 121, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompt_values import ChatPromptValue\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an AI assistant.\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "initial_answer_chain = prompt_template | llm.bind_tools(tools=tools).with_config(\n",
    "    run_name=\"GenerateInitialCandidate\"\n",
    ")\n",
    "\n",
    "parser = JsonOutputToolsParser(return_id=True)\n",
    "initial_response = initial_answer_chain.invoke(\n",
    "    {\"input\": \"Write a research report on lithium pollution.\"}\n",
    ")\n",
    "initial_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "    \"AIMessage\": {\n",
    "        \"content\": \"\",\n",
    "        \"additional_kwargs\": {\n",
    "            \"tool_calls\": [\n",
    "                {\n",
    "                    \"id\": \"call_deJYRta1OC207ayRQkKz0j1B\",\n",
    "                    \"function\": {\n",
    "                        \"arguments\": \"{\\\"query\\\":\\\"lithium pollution environmental impact research 2023\\\"}\",\n",
    "                        \"name\": \"tavily_search_results_json\"\n",
    "                    },\n",
    "                    \"type\": \"function\"\n",
    "                }\n",
    "            ],\n",
    "            \"refusal\": null\n",
    "        },\n",
    "        \"response_metadata\": {\n",
    "            \"token_usage\": {\n",
    "                \"completion_tokens\": 28,\n",
    "                \"prompt_tokens\": 93,\n",
    "                \"total_tokens\": 121,\n",
    "                \"completion_tokens_details\": {\n",
    "                    \"accepted_prediction_tokens\": 0,\n",
    "                    \"audio_tokens\": 0,\n",
    "                    \"reasoning_tokens\": 0,\n",
    "                    \"rejected_prediction_tokens\": 0\n",
    "                },\n",
    "                \"prompt_tokens_details\": {\n",
    "                    \"audio_tokens\": 0,\n",
    "                    \"cached_tokens\": 0\n",
    "                }\n",
    "            },\n",
    "            \"model_name\": \"gpt-4o-2024-08-06\",\n",
    "            \"system_fingerprint\": \"fp_d8864f8b6b\",\n",
    "            \"id\": \"chatcmpl-BPMKSBD5xdVxm9JuspQ5aIMvYuTvf\",\n",
    "            \"finish_reason\": \"tool_calls\",\n",
    "            \"logprobs\": null\n",
    "        },\n",
    "        \"id\": \"run-0366564b-fb66-492f-9b4b-cf8dd2e17cd7-0\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"tavily_search_results_json\",\n",
    "                \"args\": {\n",
    "                    \"query\": \"lithium pollution environmental impact research 2023\"\n",
    "                },\n",
    "                \"id\": \"call_deJYRta1OC207ayRQkKz0j1B\",\n",
    "                \"type\": \"tool_call\"\n",
    "            }\n",
    "        ],\n",
    "        \"usage_metadata\": {\n",
    "            \"input_tokens\": 93,\n",
    "            \"output_tokens\": 28,\n",
    "            \"total_tokens\": 121,\n",
    "            \"input_token_details\": {\n",
    "                \"audio\": 0,\n",
    "                \"cache_read\": 0\n",
    "            },\n",
    "            \"output_token_details\": {\n",
    "                \"audio\": 0,\n",
    "                \"reasoning\": 0\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the node we will add to the graph\n",
    "def generate_initial_response(state: TreeState) -> dict:\n",
    "    \"\"\"Generate the initial candidate response.\"\"\"\n",
    "    res = initial_answer_chain.invoke({\"input\": state[\"input\"]})\n",
    "    parsed = parser.invoke(res)\n",
    "    tool_responses = [\n",
    "        tool_node.invoke(\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    AIMessage(\n",
    "                        content=\"\",\n",
    "                        tool_calls=[\n",
    "                            {\"name\": r[\"type\"], \"args\": r[\"args\"], \"id\": r[\"id\"]}\n",
    "                        ],\n",
    "                    )\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        for r in parsed\n",
    "    ]\n",
    "    output_messages = [res] + [tr[\"messages\"][0] for tr in tool_responses]\n",
    "    reflection = reflection_chain.invoke(\n",
    "        {\"input\": state[\"input\"], \"candidate\": output_messages}\n",
    "    )\n",
    "    root = Node(output_messages, reflection=reflection)\n",
    "    return {\n",
    "        **state,\n",
    "        \"root\": root,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This generates N candidate values\n",
    "# for a single input to sample actions from the environment\n",
    "def generate_candidates(messages: ChatPromptValue, config: RunnableConfig):\n",
    "    n = config[\"configurable\"].get(\"N\", 5)\n",
    "    bound_kwargs = llm.bind_tools(tools=tools).kwargs\n",
    "    chat_result = llm.generate(\n",
    "        [messages.to_messages()],\n",
    "        n=n,\n",
    "        callbacks=config[\"callbacks\"],\n",
    "        run_name=\"GenerateCandidates\",\n",
    "        **bound_kwargs,\n",
    "    )\n",
    "    return [gen.message for gen in chat_result.generations[0]]\n",
    "\n",
    "\n",
    "expansion_chain = prompt_template | generate_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_mtjPL7h0h2YAeSwIY8U4krwG', 'function': {'arguments': '{\"query\":\"lithium pollution research 2023\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'finish_reason': 'tool_calls', 'logprobs': None}, id='run-5c52bdfe-2eff-415d-a09d-df1272212667-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'lithium pollution research 2023'}, 'id': 'call_mtjPL7h0h2YAeSwIY8U4krwG', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 129, 'total_tokens': 222, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_eEzazPhsbh13NObVARwuy7YX', 'function': {'arguments': '{\"query\":\"lithium pollution research report\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'logprobs': None}, id='run-5c52bdfe-2eff-415d-a09d-df1272212667-1', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'lithium pollution research report'}, 'id': 'call_eEzazPhsbh13NObVARwuy7YX', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 129, 'total_tokens': 222, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_z3Za6Ferb1kyfTjIBmey6GmW', 'function': {'arguments': '{\"query\":\"Lithium pollution research report 2023\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'logprobs': None}, id='run-5c52bdfe-2eff-415d-a09d-df1272212667-2', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'Lithium pollution research report 2023'}, 'id': 'call_z3Za6Ferb1kyfTjIBmey6GmW', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 129, 'total_tokens': 222, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_UNiZlo365zfKjICV6KUyEfzi', 'function': {'arguments': '{\"query\":\"lithium pollution research report 2023\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'logprobs': None}, id='run-5c52bdfe-2eff-415d-a09d-df1272212667-3', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'lithium pollution research report 2023'}, 'id': 'call_UNiZlo365zfKjICV6KUyEfzi', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 129, 'total_tokens': 222, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_6IPmBOxetczY4uwA4mO9wGbi', 'function': {'arguments': '{\"query\":\"lithium pollution research 2023\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'logprobs': None}, id='run-5c52bdfe-2eff-415d-a09d-df1272212667-4', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'lithium pollution research 2023'}, 'id': 'call_6IPmBOxetczY4uwA4mO9wGbi', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 129, 'total_tokens': 222, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = expansion_chain.invoke({\"input\": \"Write a research report on lithium pollution.\"})\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\n",
    "    {\n",
    "        \"content\": \"\",\n",
    "        \"additional_kwargs\": {\n",
    "            \"tool_calls\": [\n",
    "                {\n",
    "                    \"id\": \"call_mtjPL7h0h2YAeSwIY8U4krwG\",\n",
    "                    \"function\": {\n",
    "                        \"arguments\": \"{\\\"query\\\":\\\"lithium pollution research 2023\\\"}\",\n",
    "                        \"name\": \"tavily_search_results_json\"\n",
    "                    },\n",
    "                    \"type\": \"function\"\n",
    "                }\n",
    "            ],\n",
    "            \"refusal\": null\n",
    "        },\n",
    "        \"response_metadata\": {\n",
    "            \"finish_reason\": \"tool_calls\",\n",
    "            \"logprobs\": null\n",
    "        },\n",
    "        \"id\": \"run-5c52bdfe-2eff-415d-a09d-df1272212667-0\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"tavily_search_results_json\",\n",
    "                \"args\": {\n",
    "                    \"query\": \"lithium pollution research 2023\"\n",
    "                },\n",
    "                \"id\": \"call_mtjPL7h0h2YAeSwIY8U4krwG\",\n",
    "                \"type\": \"tool_call\"\n",
    "            }\n",
    "        ],\n",
    "        \"usage_metadata\": {\n",
    "            \"input_tokens\": 93,\n",
    "            \"output_tokens\": 129,\n",
    "            \"total_tokens\": 222,\n",
    "            \"input_token_details\": {\n",
    "                \"audio\": 0,\n",
    "                \"cache_read\": 0\n",
    "            },\n",
    "            \"output_token_details\": {\n",
    "                \"audio\": 0,\n",
    "                \"reasoning\": 0\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\",\n",
    "        \"additional_kwargs\": {\n",
    "            \"tool_calls\": [\n",
    "                {\n",
    "                    \"id\": \"call_eEzazPhsbh13NObVARwuy7YX\",\n",
    "                    \"function\": {\n",
    "                        \"arguments\": \"{\\\"query\\\":\\\"lithium pollution research report\\\"}\",\n",
    "                        \"name\": \"tavily_search_results_json\"\n",
    "                    },\n",
    "                    \"type\": \"function\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"response_metadata\": {\n",
    "            \"finish_reason\": \"tool_calls\",\n",
    "            \"logprobs\": null\n",
    "        },\n",
    "        \"id\": \"run-5c52bdfe-2eff-415d-a09d-df1272212667-1\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"tavily_search_results_json\",\n",
    "                \"args\": {\n",
    "                    \"query\": \"lithium pollution research report\"\n",
    "                },\n",
    "                \"id\": \"call_eEzazPhsbh13NObVARwuy7YX\",\n",
    "                \"type\": \"tool_call\"\n",
    "            }\n",
    "        ],\n",
    "        \"usage_metadata\": {\n",
    "            \"input_tokens\": 93,\n",
    "            \"output_tokens\": 129,\n",
    "            \"total_tokens\": 222,\n",
    "            \"input_token_details\": {\n",
    "                \"audio\": 0,\n",
    "                \"cache_read\": 0\n",
    "            },\n",
    "            \"output_token_details\": {\n",
    "                \"audio\": 0,\n",
    "                \"reasoning\": 0\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\",\n",
    "        \"additional_kwargs\": {\n",
    "            \"tool_calls\": [\n",
    "                {\n",
    "                    \"id\": \"call_z3Za6Ferb1kyfTjIBmey6GmW\",\n",
    "                    \"function\": {\n",
    "                        \"arguments\": \"{\\\"query\\\":\\\"Lithium pollution research report 2023\\\"}\",\n",
    "                        \"name\": \"tavily_search_results_json\"\n",
    "                    },\n",
    "                    \"type\": \"function\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"response_metadata\": {\n",
    "            \"finish_reason\": \"tool_calls\",\n",
    "            \"logprobs\": null\n",
    "        },\n",
    "        \"id\": \"run-5c52bdfe-2eff-415d-a09d-df1272212667-2\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"tavily_search_results_json\",\n",
    "                \"args\": {\n",
    "                    \"query\": \"Lithium pollution research report 2023\"\n",
    "                },\n",
    "                \"id\": \"call_z3Za6Ferb1kyfTjIBmey6GmW\",\n",
    "                \"type\": \"tool_call\"\n",
    "            }\n",
    "        ],\n",
    "        \"usage_metadata\": {\n",
    "            \"input_tokens\": 93,\n",
    "            \"output_tokens\": 129,\n",
    "            \"total_tokens\": 222,\n",
    "            \"input_token_details\": {\n",
    "                \"audio\": 0,\n",
    "                \"cache_read\": 0\n",
    "            },\n",
    "            \"output_token_details\": {\n",
    "                \"audio\": 0,\n",
    "                \"reasoning\": 0\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\",\n",
    "        \"additional_kwargs\": {\n",
    "            \"tool_calls\": [\n",
    "                {\n",
    "                    \"id\": \"call_UNiZlo365zfKjICV6KUyEfzi\",\n",
    "                    \"function\": {\n",
    "                        \"arguments\": \"{\\\"query\\\":\\\"lithium pollution research report 2023\\\"}\",\n",
    "                        \"name\": \"tavily_search_results_json\"\n",
    "                    },\n",
    "                    \"type\": \"function\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"response_metadata\": {\n",
    "            \"finish_reason\": \"tool_calls\",\n",
    "            \"logprobs\": null\n",
    "        },\n",
    "        \"id\": \"run-5c52bdfe-2eff-415d-a09d-df1272212667-3\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"tavily_search_results_json\",\n",
    "                \"args\": {\n",
    "                    \"query\": \"lithium pollution research report 2023\"\n",
    "                },\n",
    "                \"id\": \"call_UNiZlo365zfKjICV6KUyEfzi\",\n",
    "                \"type\": \"tool_call\"\n",
    "            }\n",
    "        ],\n",
    "        \"usage_metadata\": {\n",
    "            \"input_tokens\": 93,\n",
    "            \"output_tokens\": 129,\n",
    "            \"total_tokens\": 222,\n",
    "            \"input_token_details\": {\n",
    "                \"audio\": 0,\n",
    "                \"cache_read\": 0\n",
    "            },\n",
    "            \"output_token_details\": {\n",
    "                \"audio\": 0,\n",
    "                \"reasoning\": 0\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\",\n",
    "        \"additional_kwargs\": {\n",
    "            \"tool_calls\": [\n",
    "                {\n",
    "                    \"id\": \"call_6IPmBOxetczY4uwA4mO9wGbi\",\n",
    "                    \"function\": {\n",
    "                        \"arguments\": \"{\\\"query\\\":\\\"lithium pollution research 2023\\\"}\",\n",
    "                        \"name\": \"tavily_search_results_json\"\n",
    "                    },\n",
    "                    \"type\": \"function\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"response_metadata\": {\n",
    "            \"finish_reason\": \"tool_calls\",\n",
    "            \"logprobs\": null\n",
    "        },\n",
    "        \"id\": \"run-5c52bdfe-2eff-415d-a09d-df1272212667-4\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"tavily_search_results_json\",\n",
    "                \"args\": {\n",
    "                    \"query\": \"lithium pollution research 2023\"\n",
    "                },\n",
    "                \"id\": \"call_6IPmBOxetczY4uwA4mO9wGbi\",\n",
    "                \"type\": \"tool_call\"\n",
    "            }\n",
    "        ],\n",
    "        \"usage_metadata\": {\n",
    "            \"input_tokens\": 93,\n",
    "            \"output_tokens\": 129,\n",
    "            \"total_tokens\": 222,\n",
    "            \"input_token_details\": {\n",
    "                \"audio\": 0,\n",
    "                \"cache_read\": 0\n",
    "            },\n",
    "            \"output_token_details\": {\n",
    "                \"audio\": 0,\n",
    "                \"reasoning\": 0\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def select(root: Node) -> dict:\n",
    "    \"\"\"Starting from the root node a child node is selected at each tree level until a leaf node is reached.\"\"\"\n",
    "\n",
    "    if not root.children:\n",
    "        return root\n",
    "\n",
    "    node = root\n",
    "    while node.children:\n",
    "        max_child = max(node.children, key=lambda child: child.upper_confidence_bound())\n",
    "        node = max_child\n",
    "\n",
    "    return node\n",
    "\n",
    "\n",
    "def expand(state: TreeState, config: RunnableConfig) -> dict:\n",
    "    \"\"\"Starting from the \"best\" node in the tree, generate N candidates for the next step.\"\"\"\n",
    "    root = state[\"root\"]\n",
    "    best_candidate: Node = select(root)\n",
    "    messages = best_candidate.get_trajectory()\n",
    "    # Generate N candidates from the single child candidate\n",
    "    new_candidates = expansion_chain.invoke(\n",
    "        {\"input\": state[\"input\"], \"messages\": messages}, config\n",
    "    )\n",
    "    parsed = parser.batch(new_candidates)\n",
    "    flattened = [\n",
    "        (i, tool_call)\n",
    "        for i, tool_calls in enumerate(parsed)\n",
    "        for tool_call in tool_calls\n",
    "    ]\n",
    "    tool_responses = [\n",
    "        (\n",
    "            i,\n",
    "            tool_node.invoke(\n",
    "                {\n",
    "                    \"messages\": [\n",
    "                        AIMessage(\n",
    "                            content=\"\",\n",
    "                            tool_calls=[\n",
    "                                {\n",
    "                                    \"name\": tool_call[\"type\"],\n",
    "                                    \"args\": tool_call[\"args\"],\n",
    "                                    \"id\": tool_call[\"id\"],\n",
    "                                }\n",
    "                            ],\n",
    "                        )\n",
    "                    ]\n",
    "                }\n",
    "            ),\n",
    "        )\n",
    "        for i, tool_call in flattened\n",
    "    ]\n",
    "    collected_responses = defaultdict(list)\n",
    "    for i, resp in tool_responses:\n",
    "        collected_responses[i].append(resp[\"messages\"][0])\n",
    "    output_messages = []\n",
    "    for i, candidate in enumerate(new_candidates):\n",
    "        output_messages.append([candidate] + collected_responses[i])\n",
    "\n",
    "    # Reflect on each candidate\n",
    "    # For tasks with external validation, you'd add that here.\n",
    "    reflections = reflection_chain.batch(\n",
    "        [{\"input\": state[\"input\"], \"candidate\": msges} for msges in output_messages],\n",
    "        config,\n",
    "    )\n",
    "    # Grow tree\n",
    "    child_nodes = [\n",
    "        Node(cand, parent=best_candidate, reflection=reflection)\n",
    "        for cand, reflection in zip(output_messages, reflections)\n",
    "    ]\n",
    "    best_candidate.children.extend(child_nodes)\n",
    "    # We have already extended the tree directly, so we just return the state\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "\n",
    "def should_loop(state: TreeState):\n",
    "    \"\"\"Determine whether to continue the tree search.\"\"\"\n",
    "    root = state[\"root\"]\n",
    "    if root.is_solved:\n",
    "        return END\n",
    "    if root.height > 5:\n",
    "        return END\n",
    "    return \"expand\"\n",
    "\n",
    "\n",
    "builder = StateGraph(TreeState)\n",
    "builder.add_node(\"start\", generate_initial_response)\n",
    "builder.add_node(\"expand\", expand)\n",
    "builder.add_edge(START, \"start\")\n",
    "\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"start\",\n",
    "    # Either expand/rollout or finish\n",
    "    should_loop,\n",
    "    [\"expand\", END],\n",
    ")\n",
    "builder.add_conditional_edges(\n",
    "    \"expand\",\n",
    "    # Either continue to rollout or finish\n",
    "    should_loop,\n",
    "    [\"expand\", END],\n",
    ")\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "rolled out:  1\n",
      "---\n",
      "expand\n",
      "rolled out:  2\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "question = \"Generate a table with the average size and weight, as well as the oldest recorded instance for each of the top 5 most common birds.\"\n",
    "last_step = None\n",
    "for step in graph.stream({\"input\": question}):\n",
    "    last_step = step\n",
    "    step_name, step_state = next(iter(step.items()))\n",
    "    print(step_name)\n",
    "    print(\"rolled out: \", step_state[\"root\"].height)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
